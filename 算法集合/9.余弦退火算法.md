# 余弦退火算法

## 一、核心思想
余弦退火是一种**学习率调度策略**，通过**余弦函数**使学习率从初始值平滑下降到最小值。

> 目标：**前期大胆搜索，后期精细收敛**

---

## 二、基本公式（核心）

$$
\eta_t = \eta_{min} + \frac{1}{2}\left(\eta_{max}-\eta_{min}\right)
\left(1 + \cos\left(\frac{T_{cur}}{T_{max}}\pi\right)\right)
$$
**参数含义**
- $\eta_{max}$：初始最大学习率  
- $\eta_{min}$：最小学习率  
- $T_{cur}$：当前迭代步  
- $T_{max}$：总迭代步数  

---

## 三、变化特性（直观理解）

- 📉 **非线性下降**（非指数、非阶梯）
- 🟢 初期下降慢 → 保持探索能力  
- 🔵 中后期下降快 → 加速收敛  
- 🔴 末期趋于平稳 → 减少震荡  

> 相比 StepLR：**更平滑、无突变**

---

## 四、常见变体

### 1️⃣ Cosine Annealing
- 单次余弦下降
- 训练一次完整周期

### 2️⃣ Cosine Annealing with Warm Restart（SGDR）
- 多个余弦周期
- 每次重启提高跳出局部最优能力

---

## 五、为什么有效（创新点原理）

- ✅ 平滑更新避免梯度震荡  
- ✅ 后期小学习率提升泛化能力  
- ✅ 隐式正则化效果  
- ✅ 与 Adam / SGD / 动量法兼容性强  

---

## 六、适用场景
- 深度神经网络训练
- CNN / Transformer / GNN
- 小样本或高噪声任务
- 对收敛稳定性要求高的任务

---

## 七、与其他调度策略对比

| 方法 | 学习率变化 | 稳定性 | 调参难度 |
|----|----|----|----|
| StepLR | 阶梯下降 | 中 | 低 |
| Exponential | 指数下降 | 一般 | 中 |
| **Cosine Annealing** | 平滑下降 | **高** | **低** |

---

## 八、一句话总结
> **余弦退火：用“余弦曲线”替代“人为断崖式下降”，  
让模型在探索与收敛之间取得更优平衡。**

---
