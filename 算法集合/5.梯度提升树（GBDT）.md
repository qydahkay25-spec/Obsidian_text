1. **核心定义与应用场景**：梯度提升树（Gradient Boosting Decision Tree，GBDT）是一种基于提升思想的集成学习算法，广泛应用于回归和分类问题，尤其在数据挖掘、机器学习竞赛等场景表现出色。
2. **基本原理与核心思想**：GBDT 以决策树为基学习器，通过迭代的方式，每次拟合上一轮模型的残差（回归问题）或负梯度（分类问题），不断提升模型的预测能力。核心思想是将弱学习器组合成强学习器，使得模型能够逐步逼近真实值。
3. **关键步骤与执行流程**：
    - 初始化模型，一般预测值设为训练数据的均值（回归）或类别概率（分类）。
    - 对于每一轮迭代：
        - 计算当前模型的残差（回归）或负梯度（分类）。
        - 以残差或负梯度为标签，训练一棵新的决策树。
        - 确定新决策树的权重（学习率乘以步长）。
        - 更新模型，将新决策树按权重加到现有模型上。
    - 重复上述过程，直到达到预设的迭代次数或满足停止条件。
4. **优势与局限性**：
    - **优势**：对复杂非线性关系有很好的拟合能力，鲁棒性强，在许多数据集上表现出色。
    - **局限性**：对异常值敏感，训练时间较长，容易过拟合。
5. **适用数据类型与场景边界**：适用于数值型和类别型混合的数据。但数据量过大时训练效率低，数据噪声过多易导致过拟合。

### Python 代码实现

```python
from sklearn.ensemble import GradientBoostingRegressor
import numpy as np

# 生成一些示例数据
# X 是特征矩阵，这里随机生成 100 个样本，每个样本有 5 个特征
X = np.random.rand(100, 5)
# y 是目标值，这里根据 X 生成一些简单的线性关系加上噪声
y = 2 * X[:, 0] + 3 * X[:, 1] - 1 + np.random.randn(100)

# 创建 GradientBoostingRegressor 模型实例
# n_estimators 是迭代的次数，即树的数量
# learning_rate 是学习率，控制每次更新的步长
# max_depth 是决策树的最大深度
model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)

# 使用数据训练模型
model.fit(X, y)

# 预测新数据，这里随机生成 10 个新样本进行预测
new_X = np.random.rand(10, 5)
predictions = model.predict(new_X)
print(predictions)
```