1. **核心定义与应用场景**
    - **核心定义**：支持向量回归（Support Vector Regression，SVR）是一种基于支持向量机（SVM）理论的回归分析方法。它旨在找到一个最优的回归超平面，使得大部分数据点到该超平面的距离小于某个特定的阈值，同时使超平面的间隔最大化。
    - **应用场景**：广泛应用于预测问题，如时间序列预测（如股票价格预测、电力负荷预测）、物理量预测（如温度、湿度预测）以及在机器学习中对连续型变量的预测等场景。
2. **基本原理与核心思想**
    - **基本原理**：SVR 通过引入松弛变量 \(\xi_i\) 和 \(\xi_i^*\) 来允许一些样本点偏离回归超平面，同时引入惩罚参数 C 来平衡间隔最大化和样本偏离的程度。通过构造一个二次规划问题来求解回归函数的系数。
    - **核心思想**：在高维特征空间中找到一个最优超平面，使数据点尽量靠近这个超平面，同时使超平面与离它最近的数据点之间的间隔最大化。对于线性不可分的数据，通过核函数将数据映射到高维空间，从而实现线性可分。
3. **关键步骤与执行流程**
    - **数据预处理**：对原始数据进行归一化等处理，使不同特征处于相同的尺度范围，避免因特征尺度差异大影响模型性能。
    - **选择核函数**：根据数据特点选择合适核函数，如线性核函数（适用于线性可分数据）、多项式核函数、径向基核函数（RBF）等。
    - **构造并求解优化问题**：构建包含惩罚项和间隔项的目标函数，通过求解该二次规划问题得到支持向量和回归函数的系数。
    - **模型预测**：利用训练得到的回归函数对新数据进行预测。
4. **优势与局限性**
    - **优势**：在小样本情况下表现良好，能够有效处理高维数据，对噪声数据有一定的鲁棒性；核函数的使用使其可以处理非线性回归问题。
    - **局限性**：计算复杂度较高，训练时间较长；对参数 C 和核函数参数敏感，调参难度较大；模型解释性相对较弱。
5. **适用数据类型与场景边界**
    - **适用数据类型**：适用于数值型数据，无论是连续型还是离散型数值特征。
    - **场景边界**：适用于样本数量相对较少、数据维度较高的回归问题。当数据存在大量噪声或者数据分布极为复杂时，可能需要对数据进行额外处理或选择更复杂的模型。

### Python 代码实现

```python
import numpy as np
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# 生成一些示例数据
np.random.seed(0)
X = np.sort(5 * np.random.rand(40, 1), axis=0)
y = np.sin(X).ravel() + np.random.randn(40) * 0.1

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 创建并训练SVR模型
svr_rbf = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)
svr_rbf.fit(X_train, y_train)

# 进行预测
y_pred = svr_rbf.predict(X_test)

# 评估模型
mse = mean_squared_error(y_test, y_pred)
print(f"均方误差: {mse}")
```

支持向量回归（SVR）和支持向量机（SVM）都基于支持向量的概念，二者有以下区别：

1. **任务类型**
    - **SVM**：主要用于分类任务，目标是找到一个最优超平面，将不同类别的数据点尽可能准确地分开，使得两类数据点之间的间隔最大化。例如，判断一封邮件是垃圾邮件还是正常邮件。
    - **SVR**：专注于回归任务，旨在预测一个连续的数值。比如预测房价、股票价格等。它尝试找到一个函数，使得数据点与该函数的偏差尽可能小，同时保持函数的复杂度在一定范围内。
2. **损失函数**
    - **SVM**：经典的 SVM 使用合页损失函数（hinge loss），对于正确分类且间隔满足要求的样本，损失为 0；对于分类错误或间隔不满足要求的样本，损失为正。其目的是最小化分类错误的样本数量以及最大化分类间隔。
    - **SVR**：采用\(\epsilon\)- 不敏感损失函数。该函数认为只要预测值与真实值之间的偏差在\(\epsilon\)范围内，就不产生损失；只有当偏差超出\(\epsilon\)时，才计算损失。这种方式允许在一定误差范围内忽略样本的偏差，重点关注那些偏差较大的样本。
3. **输出结果**
    - **SVM**：输出的是数据点所属的类别标签，通常为离散值，比如 -1 和 1 分别代表两个不同类别。
    - **SVR**：输出的是一个连续的数值，即对目标变量的预测值，比如预测的温度值、销售额等。
4. **求解目标**
    - **SVM**：通过求解一个二次规划问题，寻找能够最大化分类间隔的超平面的参数，同时满足所有样本点的分类约束条件。
    - **SVR**：同样求解二次规划问题，但目标是在满足大部分样本点的偏差在\(\epsilon\)范围内的同时，最小化模型的复杂度，以避免过拟合。这里复杂度通常通过正则化项来控制，与 SVM 中最大化间隔有不同的侧重点。
5. **模型评估指标**
    - **SVM**：常用准确率、精确率、召回率、F1 值等指标来评估模型的分类性能，这些指标反映了模型对不同类别预测的准确程度。
    - **SVR**：一般使用均方误差（MSE）、均方根误差（RMSE）、平均绝对误差（MAE）等指标来衡量预测值与真实值之间的误差大小，以评估回归模型的性能。

